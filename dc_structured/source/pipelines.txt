# Pipelines

This module proposes several workflows that greatly simplify deep learning research on CT-scans. Each workflow is represented in a form of several preprocessing operations ([actions](https://github.com/analysiscenter/dataset)), chained in a [pipeline](https://github.com/analysiscenter/dataset).

Let us start with a workflow that allows to perform a full-scale preprocessing over a dataset of scans and start training the model of your choice.

## Preprocessing workflow.
The workflow, that includes load of data from disk, resize to shape **[128, 256, 256]**, and preparing batch of **20** cancerous and non-cancerous crops of shape **[32, 64, 64]**, can be set up in a following way:
```python
from lung_cancer import get_preprocessed # function that build a pipeline
pipe = get_preprocessed(fmt='raw', shape=(128, 256, 256), nodules_df=nodules, batch_size=20,
                        share=0.6, nodule_shape=(32, 64, 64))
```
Pay attention to parameters `batch_size` and `share`: they allow to control the number of items in a batch of crops and the number of cancerous crops. We can then chain `pipe` with more actions for training, say, [DenseNet](linkondense):
``` python
pipe = pipe.train_model(model='vnet', model_class=DenseNetModel)
(pipe >> ctset).run(BATCH_SIZE=12)
```
Alternatively, we can choose to save the prepared dataset of crops on disk and get back to training a net on it later:
``` python
pipe = pipe.dump('/path/to/crops/')
(pipe >> ctset).run(BATCH_SIZE=12)
```
Worth noting that the defined pipeline will generate `~1500` training examples, when run one time through the Luna-dataset (one epoch). In the same time, it may take a couple of hours to work through the pipeline, even for a high performing machine. The reason for this is that both `resize` and `load` are costly operations.

That being said, for implementing an efficient learning procedure we advise to use another workflow, that allows to generate more than `100000` training examples after running one time through the Luna-dataset.

## A faster workflow.

A preparation of a richer training dataset can be achieved in two steps. During the first step we dump cancerous and non-cancerous crops on a disk. The procedure goes as follows:
```python
from lung_cancer import split_dump
pipe = split_dump()
```
