{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of CT-scans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial you will be able to:\n",
    "1. Load CT-scans from MetaImage (mhd) format\n",
    "2. Run preprocessing and dump scans to [blosc](https://github.com/Blosc/python-blosc)\n",
    "    1. Resize all scan to fixed size\n",
    "    2. Unify spacing AND resize to fixed size\n",
    "3. Load dumped files and make masks for them\n",
    "4. Visualize slices of scans\n",
    "5. Sample crops of fixed size from preprocessed scans with masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples in this notebook use [LUNA16 competition dataset](https://luna16.grand-challenge.org/) in MetaImage (mhd/raw) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import shutil\n",
    "from ipywidgets import interact\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from radio import CTImagesMaskedBatch as CTIMB\n",
    "from radio.dataset import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load CT-scans from MetaImage (mhd) format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to specify mask for '\\*.mhd' input files in DIR_LUNA, and provide output dir path in DIR_DUMP. Here we use unzipped competition dataset, mhd files are stored in subfolders, names of subfolders are taken as ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIR_LUNA = '/notebooks/data/MRT/luna/s*/*.mhd'\n",
    "DIR_DUMP = '/notebooks/data/MRT/output/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**: think thoroughly before running the cell below, it deletes output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(DIR_DUMP):\n",
    "    shutil.rmtree(DIR_DUMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by creating  `Dataset.FilesIndex` and  `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = FilesIndex(path=DIR_LUNA, no_ext=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is ok, you'll see total number of mhd files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(ind.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds = Dataset(index=ind, batch_class=CTIMB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run preprocess on dataset and dump it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Reshaping dataset to fixed shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, worlflow is in lazy-mode, so it is not yet running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "workflow = (\n",
    "    ds.pipeline()\n",
    "      .load(fmt='raw')\n",
    "      .resize(n_workers=6, shape=(128, 256, 256))\n",
    "      .dump(dst=DIR_DUMP)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you actually start preprocessing.\n",
    "\n",
    "Note, that preprocessing all LUNA16 scans may take significant time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "workflow.run(batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Unify spacing AND resize to fixed size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this goal pipeline would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "workflow = (\n",
    "    ds.pipeline()\n",
    "      .load(fmt='raw')\n",
    "      .unify_spacing(shape=(384, 448, 448), spacing=(0.9, 0.9, 0.9))\n",
    "      .dump(dst=DIR_DUMP)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea is following: \n",
    "\n",
    "1) Shape is changed for every scan so, that spacing would meet required **```(0.9, 0.9, 0.9)```**\n",
    "\n",
    "2) Interim shape is cropped (if it is bigger) or padded (if it is smaller) to meet **```shape```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load dumped scans, build masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you need annotation file with nodules locations and diameters. \n",
    "\n",
    "It is also provided by LUNA16 https://luna16.grand-challenge.org/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodules = pd.read_csv('/notebooks/data/MRT/luna/CSVFILES/annotations.csv')\n",
    "\n",
    "ind_dumped = FilesIndex(path=DIR_DUMP + '*', dirs=True)\n",
    "\n",
    "batch_dumped = CTIMB(ind_dumped.create_subset(ind_dumped.index[0 : 4]))\n",
    "\n",
    "batch_dumped.load(fmt='blosc')\n",
    "\n",
    "batch_dumped.fetch_nodules_info(nodules)\n",
    "\n",
    "batch_dumped.create_mask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you just loaded 1 batch of 4 scans and made masks for it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Check the whole thing: visualise slices of scans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is convenient to use interact for visualising various slices and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_arr_slices(height, *arrays, clim=(-1200, 300)):\n",
    "    fig, axes = plt.subplots(1, len(arrays), figsize=(14, len(arrays)*8))\n",
    "    \n",
    "    for arr, i in zip(arrays, range(len(arrays))):\n",
    "        depth = arr.shape[0]\n",
    "        n_slice = int(depth * height)\n",
    "        \n",
    "        kwargs = dict()\n",
    "        if np.max(arr) - np.min(arr) > 2.0:\n",
    "            kwargs.update(clim=clim)\n",
    "        else:\n",
    "            kwargs.update(clim=(0, 1))\n",
    "    \n",
    "        axes[i].imshow(arr[n_slice], cmap=plt.cm.gray, **kwargs)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the first patient scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_pat = 0\n",
    "\n",
    "interact(lambda height: plot_arr_slices(height, batch_dumped[n_pat], batch_dumped.get_mask(n_pat)), \n",
    "         height=(0.01, 0.99, 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Sample crops of fixed size from preprocessed scans with masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take batch we loaded and visualised and sample crops with nodules from it via ```sample_nodules``` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nods_batch = batch_masked.sample_nodules(batch_size=10, nodule_size=(32, 64, 64), share=0.7,\n",
    "                                         variance=[100, 400, 400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It creates new batch with 10 items in it, which are crops (aka patches) from original scans of size (32, 64, 64). \n",
    "\n",
    "However, you may want crops with nodules in different positions (not in the center of crop), for this specify ```variance``` which allows to shift center of crop to (10, 20, 20) voxels along (z, y, x) axes in example above. \n",
    "\n",
    "Also, for traingin neural nets, you may want to have crops without nodules at all, specify ```share``` which is share of items with nodules."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
