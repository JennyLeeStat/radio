{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append('./lung_cancer/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training models for pulmonary nodule detection on LUNA16 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Tutorial you will learn how to train models on popular [LUNA16](http://luna16.grand-challenge.org) competition dataset (scans from [LIDC-IDRI](https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI).\n",
    "You will learn how to:\n",
    "1. Prepare dataset of CT-scans crops of smaller size\n",
    "2. Train a ready-made model from recommended RadIO model zoo, e.g. [**Keras3DUnet**](https://analysiscenter.github.io/radio/api/keras_3dunet.html)\n",
    "3. Train a popular model from dataset/models zoo, e.g. [**V-Net**](https://analysiscenter.github.io/dataset/api/dataset.models.tf.vnet.html)\n",
    "4. Write your own model on TensorFlow and train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I. Prepare dataset of crops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is not yet practically approachable to train a neural network on CT-scans, as they weight too much (300-500mb+), one approach which allows to tackle volumetric information would be to train on smaller 3D parts, `crops`. Cropping parts with/without nodules also allows for better sampling and balance of positive/negative examples in batch. RadIO contains ready-made preprocessing pipeline which will create a dataset with crops, you may use LUNA16 challenge dataset in MetaImage format for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from radio.pipelines import split_dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need LUNA16 folder directory and `annotations.csv` provided for the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "nodules = pd.read_csv('/data/MRT/luna/CSVFILES/annotations.csv')\n",
    "DIR_LUNA = '/data/MRT/luna/s*/*.mhd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from radio.dataset import FilesIndex, Dataset\n",
    "from radio import CTImagesMaskedBatch as CTIMB\n",
    "\n",
    "lunaix = FilesIndex(path=DIR_LUNA, no_ext=True)\n",
    "lunaset = Dataset(index=lunaix, batch_class=CTIMB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that you may also want to split your dataset to train/test/validation parts; It's possible by splitting indices of dataset instance by `.cv_split` method, for example, splitting 90% to train and 10% for test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lunaset.cv_split(0.9, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now original dataset is split to train and test sub-datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(799, 89)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lunaset.train), len(lunaset.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to write down preprocessing workflow. \n",
    "\n",
    "**`split_dump()`** from `radio.pipelines` returns basic workflow, including normalizing Hounsfield Units, resizing all scans to fixed scans and in at same time unifying its spacing to desired one. After preprocessing `split_dump` will sample smaller crops of desired shape (32, 64, 64) with nodules and without. Workflow will also create masks based on nodules location and dump all the component, e.g. `masks`, `images`, into fast [`blosc`](http://blosc.org/pages/blosc-in-depth/) format.\n",
    "\n",
    "Crops with nodules will be dumped to `cancer_path` and crops without any nodules will be dumped to `non_cancer_path` to allow managing balance of positive/negative examples in batch later during training.\n",
    "\n",
    "You may want to pass additioanl paramteres as `kwargs`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SPACING = (1.7, 1.0, 1.0)  # spacing of scans after spacing unification\n",
    "SHAPE = (400, 512, 512)  # shape of scans after spacing unification\n",
    "PADDING = 'reflect'  # padding-mode that produces the least amount of artefacts\n",
    "METHOD = 'pil-simd'  # robust resize-engine\n",
    "\n",
    "kwargs_default = dict(shape=SHAPE, spacing=SPACING, padding=PADDING, method=METHOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crop_pipeline = split_dump(cancer_path='/data/lunaset_split/train/cancer/', \n",
    "                           non_cancer_path='/data/lunaset_split/train/ncancer/',\n",
    "                           nodules=nodules, fmt='raw', nodule_shape=(32, 64, 64), batch_size=20, **kwargs_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass dataset directly to pipeline and run it, for example for `lunaset.train` sub-dataset:\n",
    "\n",
    "(Note, it may take some time, so better grab a coffee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lunaset.train >> crop_pipeline).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to prepare test part in similar way, but do not forget to change `cancer_path` and `non_cancer_path`. In this Tutorial we will skip it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II. Train a ready made model in Keras: 3D U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RadIO contains a small but growing zoo of models in Keras and Tensorflow. In this part you will perform training a 3D U-Net architecture for pulmonary nodule segmentation, training on crops you prepared earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIR_CANCER = '/data/lunaset_split_new/train/cancer/*'\n",
    "DIR_NCANCER = '/data/lunaset_split_new/train/ncancer/*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to preprocessing, let's build a training pipeline. This time, you will need 2 datasets, one for crops with nodules and another with crops without them, that's why they were dumped in different folders. Remeber, that masks and some other special information is also dumped and will be easily loaded on request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cix = FilesIndex(path=DIR_CANCER, dirs=True)\n",
    "ncix = FilesIndex(path=DIR_NCANCER, dirs=True)\n",
    "\n",
    "cancerset = Dataset(index=cix, batch_class=CTIMB)\n",
    "ncancerset = Dataset(index=ncix, batch_class=CTIMB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we use both datasets in a same time during training? Dataset allows to `merge` them specifying batch_size for each of them. Say, we would want take 4 crops with nodules and 4 without them, in total batch_size of 8 crops of (32, 64, 64) size. Let's import a ready made pipeline `combine_crops` for this problem and specify parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from radio.pipelines import combine_crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combine_pipeline = combine_crops(cancerset, ncancerset, batch_sizes=(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step, the model: import and define config in a dict. Need to specify input crops and masks sizes (will be same) as well as trainging model config: Optimizer, Loss function; e.g. use Adam for training and Dice loss funciton for segmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from radio.models import Keras3DUNet\n",
    "from radio.models.keras.losses import dice_loss\n",
    "\n",
    "model_config = dict(\n",
    "    input_shape = (1, 32, 64, 64),\n",
    "    num_targets = 1,\n",
    "    optimizer ='Adam',\n",
    "    loss= dice_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, let's add one more part to training pipeline specifying model and training params by chaining `.init_model` and `.train_model` pipeline methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from radio.dataset import F\n",
    "\n",
    "train_pipeline = (\n",
    "    combine_pipeline\n",
    "    .init_model('static', Keras3DUNet,name='3dunet',config=model_config)\n",
    "    .train_model('3dunet',x=F(CTIMB.unpack, component='images',\n",
    "                              data_format='channels_first'),\n",
    "                          y= F(CTIMB.unpack, component='segmentation_targets',\n",
    "                              data_format='channels_first'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `train_model` employ custom `CTIMB.unpack` method that helps to unpack specific components from batch and put them into desired alias of keras network: `images` component goes to `x` and `segmentation_targets` put `masks` into `y`.\n",
    "\n",
    "If you would want to try a keras network for classification, you could simply pass component for y's unpack to `classification_targets`. That way, unpack would transform masks into binary classification (`nodule` vs. `non-nodule`) target based on threshold of amount of cancer voxels inside the crop to label `nodule`. See [documentation](https://analysiscenter.github.io/radio/intro/models.html) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the moment of truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After execution is finished, you may `.save` keras model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unet = train_pipeline.get_model_by_name('3dunet')\n",
    "unet.save('3dunet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you later would like to access it, that would be easy by adding `path` in `model_config` and redefining and running same pipeline again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config.update({'path': '3dunet'})\n",
    "\n",
    "train_pipeline = (\n",
    "    combine_pipeline\n",
    "    .init_model('static', Keras3DUNet, name='3dunet',config=model_config)\n",
    "    .train_model('3dunet',x=F(CTIMB.unpack, component='images',\n",
    "                              data_format='channels_first'),\n",
    "                          y=F(CTIMB.unpack, component='segmentation_targets',\n",
    "                              data_format='channels_first'))\n",
    ")\n",
    "\n",
    "train_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Training a model from radio.dataset.models zoo: V-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset library used by RadIO contains a zoo of many popular model architecture in tensorflow. In this part you will learn to train V-Net (volumetric u-net) for same task with same preprocessing pipeline. All you need is to define config of dataset's v-net model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from radio.dataset.models.tf import VNet\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_config = dict(\n",
    "    images={'shape': (32, 64, 64, 1)},\n",
    "    labels={'name': 'targets', 'shape': (32, 64, 64, 1)}\n",
    ")\n",
    "\n",
    "\n",
    "model_config = dict(\n",
    "    inputs=inputs_config,\n",
    "    optimizer='Adam',\n",
    "    loss='sigmoid_cross_entropy',\n",
    ")\n",
    "\n",
    "model_config['input_block/inputs'] = 'images'\n",
    "model_config['head/num_classes'] = 1\n",
    "model_config['head/layout'] = 'cna'\n",
    "model_config['head/kernel_size'] = 1\n",
    "model_config['head/activation'] = tf.nn.sigmoid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset.models has highly parametrized architecures which allows to perform experiments by combining different paramteres without changing model's code. It greatly facilitates clear and reproducible experiment settings.\n",
    "\n",
    "In example above you are provided with V-Net model which will be trained on crops with Adam optimizer, sigmoid_cross_entropy loss from tensorflow for binary classification. Slight modifications are added to head of the network, last convolutional block would consist of convolution with (1, 1, 1) kernel, 1 filter, following batch normalisation layer and sigmoid activation function. See dataset [documentation](https://analysiscenter.github.io/dataset/intro/tf_models.html) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from radio.dataset import F\n",
    "\n",
    "vnet_ppl = (\n",
    "    combine_pipeline\n",
    "    .init_model('static', VNet, 'vnet', config=model_config)\n",
    "    .train_model('vnet',feed_dict={'images':  F(CTIMB.unpack, component='images',data_format='channels_last'),\n",
    "                                   'targets': F(CTIMB.unpack, component='segmentation_targets')})\n",
    ")\n",
    "\n",
    "vnet_ppl.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Part 4. Write you own tensorflow model and train it with same pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part you will see, how to train your own models ith RadIO. The easiest way would be to write your model in tensorflow or keras and use `.import_model()` in pipeline. Let's build a 3D convolutional encoder-decoder. architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to be able to use `pipeline` would be to inherit `BaseModel` class and redefine its **`build`** method for building model's graph in tensorflow and also **`train`** method as it will be used by pipeline's `.train_model`. See documentation of [BaseModel](https://analysiscenter.github.io/dataset/api/dataset.models.html#dataset.models.BaseModel) to make sense of its possibilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from radio.dataset.models import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_model(BaseModel):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.inputs = None\n",
    "        self.targets = None\n",
    "        self.output = None\n",
    "        self.loss = None\n",
    "        self.sess = None\n",
    "        self.train_step = None\n",
    "        self.graph = tf.Graph()\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def build(self, *args, **kwargs):\n",
    "        filters = [32, 64, 128, 256]\n",
    "        with self.graph.as_default():\n",
    "            with tf.variable_scope('custom', reuse=False):\n",
    "                \n",
    "                self.inputs = tf.placeholder(shape=(None, 32, 64, 64, 1), dtype=tf.float32)\n",
    "                self.targets = tf.placeholder(shape=(None, 32, 64, 64, 1), dtype=tf.float32)\n",
    "                \n",
    "                x = self.inputs\n",
    "                for f in filters:\n",
    "                    x = tf.layers.conv3d(x, f, (3, 3, 3), padding='same', activation=tf.nn.relu)\n",
    "                    x = tf.layers.conv3d(x, f, (3, 3, 3), padding='same', activation=tf.nn.relu)\n",
    "                    x = tf.layers.max_pooling3d(x, pool_size=(2, 2, 2), strides=(2, 2, 2), padding='same')\n",
    "\n",
    "                x = tf.layers.conv3d(x, f*2, (3, 3, 3), padding='same', activation=tf.nn.relu)\n",
    "                x = tf.layers.conv3d(x, f*2, (3, 3, 3), padding='same', activation=tf.nn.relu)\n",
    "\n",
    "                for f in filters[::-1]:\n",
    "                    x = tf.layers.conv3d_transpose(x, f, (2, 2, 2), (2, 2, 2), use_bias=False, padding='same')\n",
    "                    x = tf.layers.conv3d(x, f, (3, 3, 3), padding='same', activation=tf.nn.relu)\n",
    "                    x = tf.layers.conv3d(x, f, (3, 3, 3), padding='same', activation=tf.nn.relu)\n",
    "\n",
    "\n",
    "                self.output = tf.layers.conv3d(x, 1, (1, 1, 1), padding='same', activation=tf.nn.sigmoid)\n",
    "\n",
    "                self.loss = tf.losses.sigmoid_cross_entropy(self.targets, self.output)\n",
    "                self.train_step = tf.train.AdamOptimizer().minimize(self.loss)\n",
    "\n",
    "            self.sess = tf.Session()\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "    \n",
    "    def train(self, x, y): \n",
    "        self.sess.run(self.train_step, feed_dict={self.inputs: x, self.targets: y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can use `.import_model` just like that, and not forget to provide right arguments (from train method) into `.train_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from radio.dataset import F\n",
    "\n",
    "model = Custom_model()\n",
    "\n",
    "cust_ppl = (\n",
    "    combine_pipeline\n",
    "    .import_model(name='custom', source=model)\n",
    "    .train_model('custom', x = F(CTIMB.unpack, component='images',data_format='channels_last'),\n",
    "                           y = F(CTIMB.unpack, component='segmentation_targets'))\n",
    ")\n",
    "\n",
    "cust_ppl.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
